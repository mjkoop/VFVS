****** Job Resource Configuration 

job_letter=testing
# alphabetic characters (i.e. letters from a-z or A-Z)
# Used to describe distinct runs (using the same name will overwrite data)

batchsystem=awsbatch
# Possible values: awsbatch

threads_to_use=8
# This sets how many processes the main execution loop should be using
# to process. This is generally 2x the number of vCPUs or hyperthreads
# available on the system it is being run on

program_timeout=90
# How many seconds to wait for each ligand to be processed by a program

job_storage_mode=s3
# This mode determines where data is retrieved and stored from as part of 
# VFVS. Valid modes:
#   * s3: Job data is stored on S3 object store, which is the recommended
#         mode if using AWS Batch. Items under the "S3 Object Store" 
#         heading in the configuration are required if this mode is used
#   * sharedfs: This mode requires that all running jobs have access to the
#         same shared filesystem that will allow for both input and output
#         of data


****** AWS Batch Options

### To use AWS Batch you must first complete the steps outlined 
### in the user guide for AWS Batch 

aws_batch_prefix=vf
# Prefix for the name of the AWS Batch queues. This is normally 'vf'
# if you used the CloudFormation template

aws_batch_number_of_queues=2
# Should be set to the number of queues that are setup for AWS Batch. 
# Generally this number is 2 unless you have a large-scale (100K+ vCPUs)
# setup

aws_batch_jobdef=vf-jobdef-vfvs
# Generally this is [aws_batch_prefix]-jobdef-vfvs
# (e.g. if aws_batch_prefix=vf, then aws_batch_jobdef=vf-jobdef-vfvs

aws_batch_array_job_size=200
# Target for the number of jobs that should be in a single array job for AWS Batch.

aws_ecr_repository_name=vf-vfvs-ecr
# Set it to the name of the Elastic Container Registry (ECR) 
# repository (e.g. vf-vfvs-ecr) in your AWS account
# (If you used the template it is generally vf-vfvs-ecr)

aws_region=us-east-1
# Set to the AWS location code where you are running AWS Batch
# (e.g. us-east-1 for North America, Northern Virginia)


****** Object Store Settings

object_store_job_bucket=
# Bucket name for the job data (output and job files)

object_store_job_prefix=jobs
# Where to place job-specific data. This includes where VirtualFlow will place 
# the input data needed for jobs as well as the output files. 
#
# Data be be placed:
# if object_store_job_addressing_mode=hash
#	in object_store_job_prefix/XX/YY/<job_letter>
#       (where 'XX', and 'YY' are hash values that will vary for 
#        different files)
# else
# 	in object_store_job_prefix/<job_letter>

object_store_data_bucket=
# Bucket name for the input collection data (often the same as the job one)

object_store_data_collection_prefix=
# Prefix used within the object store to address the collections

object_store_data_collection_addressing_mode=metatranche
# If input is placed with the hash addressing mode, then use 'hash'.
# otherwise use "metatranche" for the classic addressing mode

object_store_job_addressing_mode=metatranche
# If job output is to be placed with the hash addressing mode, then use 'hash'.
# otherwise use "metatranche" for the classic addressing mode

object_store_data_collection_identifier=
# This is only used if object_store_data_collection_addressing_mode=hash
# Generally this is the dataset name (e.g. Enamine_REAL_Space_2021q12)


****** Workflow Options

ligands_todo_per_queue=1000
# Used as a limit of ligands for the to-do lists. 
# A reasonable number for this is generally 1000. The length of time
# to process will depend on the docking scenarios run

collection_folder=../ligand-library
# Slash at the end is not required (optional)
# Relative pathname is required w.r.t. the folder tools/
# This is not used unless you are using the helper script to upload

ligand_library_format=pdbqt
# Supported values:
#  * pdbqt
#  * mol2
# This value is case sensitive
# All AutoDock based docking programs require the library to be in the pdbqt format. #
# When the docking program PLANTS is used, both libraries in the pdbqt and the mol2 format are supported.

verbosity_commands=standard
# Possible values: standard, debug
# This option mainly effects the screen output and the logfiles
# Settable via range control files: No

verbosity_logfiles=standard
# Possible values:
#   * standard
#   * debug : activates the set -x option. Increases size of log-files in average by nearly a factor of 10
# This option affects the preparation scripts for setting up the basic workflow files (before the workflow is running)
# Settable via range control files: Yes

store_queue_log_files=all_compressed_error_uncompressed
# Supported values (experimental)
#   * all_uncompressed: requires most memory any and storage, but is recommending for test runs and debugging purposes
#   * all_compressed: requires less memory and storage than uncompressed, but during the last part of the log files might get lost (in particular during crashes) due to the on-the-fly compression
#   * only_error_uncompressed: only stderr is logged
#   * only_error_compressed: only stderr is logged and compressed. The last part of the log might get lost (in particular during crashes) due to the on-the-fly compression.
#   * std_compressed_error_uncompressed
#   * all_compressed_error_uncompressed
#   * none: reduces required memory and storage
# Settable via range control files: Yes

keep_ligand_summary_logs=true
# Summary log files which show for each ligand the success status of conversion and the conversion time.
# If the conversion failed, a reason is stated.
# If the transformation succeeded, the conversion programs which were used are stated.
# Possible values:
#   * false
#   * true
# Settable via range control files: Yes

error_sensitivity=normal
# Possible values: normal, high
# high sets the shell options "-uo pipefail". Not recommended for production runs, useful mainly for debugging. Pipefails often occur with tar combined with head/tail in pipes, which are not an actual problem.
# The u-option will always lead to a direct exit of the shell script when an unset variable is going to be used.
# Settable via range control files: Yes

error_response=fail
# Affects most errors, but not all (e.g. not the u-option of the shell)
# Possible values:
#   * ignore    : ignore error and continue
#   * next_job  : end this job and start new job
#   * fail      : exit workflow with failure (exit code 1)
# Settable via range control files: Yes

tempdir_default=/dev/shm
# The directory which is used for the temporary workflow files which need a normal performance
# Is normally a local SSD or HDD
# The directory does only need to be available on the node on which the job step/queue is running
# In the tempdir, a subfolder named ${USER} will automatically be created
# Settable via range control files: Yes

tempdir_fast=/dev/shm
# The directory which is used for the temporary workflow files which need a fast perfomance
# Should be a a local ram filesystem/ramdisk
# The directory does only need to be available on the node on which the job step/queue is running
# In the tempdir, a subfolder named ${USER} will automatically be created
# Settable via range control files: Yes

outputfiles_level=collection
# Possible values:
#   * collection  : The collection output files are stored in tar.gz format. They are stored in subfolders named by metatranch and tranch to reduce the number of files per folder.
#                   Advantages:
#                       * Less I/O on the shared cluster file system (as existing tranch archives don't have to be read during storage of completed collectionsds)
#                       * No risk of output-file clashes when two queues want to store completed collections on the shared filesystem
#   * tranche      : For each tranch a tar archive is created, which contains the gzipped collection output files.
#                   Advantages:
#                       * Less output files (only for each tranch) for each of the output file types (e.g. results, summaries, logfiles, ...)


****** Virtual Screening Options

docking_scenario_names=qvina02_rigid_receptor1
# Names for the docking scenarios, separated by colons
# Each docking scenario has one value. Multiple docking scenarios/names have to be separated by colons ":" and without spaces
# Example: docking_scenario_names=receptor1_vina_rigid:receptor1_smina_flexible
# The docking scenario names are for exapmle used for the folder names in which the output files are stored
# The value should not be changed during runtime, and be the same for all joblines

docking_scenario_programs=qvina02
# For each docking scenario name, a docking program has to be specified
# Possible values: qvina02, qvina_w, vina, smina_rigid, smina_flexible, gwovina, adfr
# Values have to be separated by colons ":" and without spaces, e.g: docking_scenario_programs=vina:smina
# smina_rigid has to be used for rigid docking with smina, while smine_flexible for flexible receptor docking
# The value should not be changed during runtime, and be the same for all joblines

docking_scenario_replicas=1
# Series of integers separated by colons ":"
# The number of values has to equal the number of docking programs specified in the variable "docking_programs"
# The values are in the same order as the docking programs specified in the variable "docking_scenario_programs
# e.g.: docking_scenario_replicas=1:1
# possible range: 1-99999 per field/docking program
# The docking scenario is comprised of all the docking types and their replicas
# The value should not be changed during runtime, and be the same for all joblines

docking_scenario_basefolder=../input-files
# Relative path to tools directory
# Base directory for where the docking scenarios are held. Nothing other 
# than the required files for the docking scenario should be placed here

docking_scenario_inputfolders=qvina02_rigid_receptor1
# folder names inside 'docking_scenario_basefolder' 
# In each input folder must be the file config.txt which is used by the docking program to specify its options
# If other input files are required by the docking type, usually specified in the config.txt file, they have to be in the same folder
# The value should not be changed during runtime, and be the same for all joblines

